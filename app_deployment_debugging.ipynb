{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd69e683",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from PIL import Image\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, root_mean_squared_error, r2_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from sklearn.decomposition import PCA\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42e4ad3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def compare_columns_df(deploy_df, target_df):\n",
    "    # Find missing columns from target_df compared to deploy_df\n",
    "    missing_from_target = set(target_df.columns) - set(deploy_df.columns)\n",
    "    \n",
    "    # Find missing columns from deploy_df compared to target_df\n",
    "    missing_from_deploy = set(deploy_df.columns) - set(target_df.columns)\n",
    "\n",
    "    # Output the missing columns and their counts\n",
    "    print(\"Missing columns from deploy_df compared to target_df:\", missing_from_target)\n",
    "    print(\"Number of missing columns from deploy_df compared to target_df:\", len(missing_from_target))\n",
    "    \n",
    "    print(\"Missing columns from target_df compared to deploy_df:\", missing_from_deploy)\n",
    "    print(\"Number of missing columns from target_df compared to deploy_df:\", len(missing_from_deploy))\n",
    "\n",
    "    # Check if the lengths of the DataFrames' columns are equal\n",
    "    print(\"Number of columns in deploy_df:\", len(deploy_df.columns))\n",
    "    print(\"Number of columns in target_df:\", len(target_df.columns))\n",
    "    \n",
    "    if len(deploy_df.columns) == len(target_df.columns):\n",
    "        print(\"The number of columns in deploy_df is equal to that in target_df.\")\n",
    "    else:\n",
    "        print(\"The number of columns in deploy_df is not equal to that in target_df.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca9a0d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "def load_dataset(path):\n",
    "    df = pd.read_csv(path)\n",
    "    if \"time\" in df.columns:\n",
    "        df[\"time\"] = pd.to_datetime(df[\"time\"], utc = True)\n",
    "    return df\n",
    "\n",
    "def manipulate_datasets(energy_df, weather_df):\n",
    "    #deal with null values\n",
    "    #Energy Features\n",
    "    energy_df[\"time\"] = pd.to_datetime(energy_df[\"time\"], utc = True)\n",
    "    weather_df[\"time\"] = pd.to_datetime(weather_df[\"time\"], utc = True)\n",
    "    energy_generation_columns = ['generation biomass', 'generation fossil brown coal/lignite',\n",
    "       'generation fossil gas', 'generation fossil hard coal',\n",
    "       'generation fossil oil', 'generation hydro pumped storage consumption',\n",
    "       'generation hydro run-of-river and poundage',\n",
    "       'generation hydro water reservoir', 'generation nuclear',\n",
    "       'generation other', 'generation other renewable', 'generation solar',\n",
    "       'generation waste', 'generation wind onshore'\n",
    "]\n",
    "    energy_df[\"total_gen\"] = energy_df[energy_generation_columns].sum(axis=1)\n",
    "\n",
    "    #Weather data features creation\n",
    "    weather_df[\"temp_diff\"] = weather_df[\"temp_max\"] - weather_df[\"temp_min\"]\n",
    "    weather_df.drop(columns=[\"temp_max\",\"temp_min\"],inplace=True)\n",
    "    weather_df[\"weather_id\"] = weather_df[\"weather_id\"].astype(str)\n",
    "    weather_df = pd.get_dummies(weather_df, columns=[\"weather_id\"], prefix=\"weather_id\")\n",
    "    #Combine the dfs\n",
    "    cities = weather_df[\"city_name\"].unique()\n",
    "    user_data = energy_df.copy()\n",
    "    for city in cities:\n",
    "        #indexes values for a certain city name and drops the column\n",
    "        df_city = weather_df[weather_df[\"city_name\"] == city]\n",
    "        df_city = df_city.drop(columns=\"city_name\")\n",
    "        \n",
    "        #Duplicate times still exist within the weather dataset with a unique city. To gain a unique time, a \n",
    "        numeric_cols = df_city.select_dtypes(include=['number']).columns\n",
    "        \n",
    "        #Assigns the mean of duplicate time value entries to a unique time value\n",
    "        df_city = df_city.groupby('time')[numeric_cols].agg('mean').reset_index()\n",
    "        \n",
    "        #Rename columns to include the city name\n",
    "        for col in numeric_cols:\n",
    "            df_city = df_city.rename(columns={col: col + \"_\" + city})\n",
    "        \n",
    "        #Merge the dfs\n",
    "        user_data = user_data.merge(df_city, on=\"time\", how=\"inner\")\n",
    "    #Drop columns as no correlation exists \n",
    "    try:\n",
    "        user_data.drop(['snow_3h_ Barcelona', 'snow_3h_Seville'], axis=1, inplace=True)\n",
    "    except:\n",
    "        raise KeyError(f\"Ensure all five cities are represented within the dataset\")\n",
    "    #Fill nan values\n",
    "    user_data = user_data.fillna(user_data.mean())\n",
    "\n",
    "    return user_data\n",
    "\n",
    "def create_engineered_features(df, feature_engineered_data): \n",
    "    #Time Feats\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    def create_time_features(df, create_time_period_feature = False, step = 6):\n",
    "        #Creates time related features of month, day, hour\n",
    "        time_feat_df = pd.DataFrame()\n",
    "        time_feat_df = df[[\"time\",\"price actual\"]]\n",
    "\n",
    "        time_feat_df[\"month\"] = time_feat_df['time'].dt.month_name()\n",
    "        time_feat_df[\"day\"] = time_feat_df['time'].dt.day_name()\n",
    "        time_feat_df[\"hour\"] = time_feat_df[\"time\"].dt.hour.astype('category')\n",
    "        #Creates stepped features based on hour\n",
    "        #using pd.getdummies creates 23 additional features, whereas hour features could be captured via a period\n",
    "        if create_time_period_feature:\n",
    "            for i in range(step, 24, step):\n",
    "                time_feat_df[f\"is_btwn_{i}_and_{i+step}\"] = time_feat_df[\"hour\"].isin(range(i, i + step)).astype(bool)\n",
    "\n",
    "            time_feat_df.drop(columns=[\"hour\"],inplace=True)\n",
    "        \n",
    "        return time_feat_df\n",
    "    time_best_feat = create_time_features(df, create_time_period_feature=True,step = 6).drop(columns=\"price actual\")\n",
    "\n",
    "#Lag and MA feats\n",
    "# Create time features\n",
    "    lag_ma_feats = create_time_features(df)\n",
    "    lag_ma_feats[\"year\"] = lag_ma_feats[\"time\"].dt.year\n",
    "    lag_ma_feats[\"week\"] = lag_ma_feats[\"time\"].dt.isocalendar().week\n",
    "    lag_ma_feats.set_index(\"time\", inplace=True)\n",
    "\n",
    "    # Compute weekly average and merge with previous years for comparison\n",
    "   \n",
    "    weekly_avg_last_year = lag_ma_feats.groupby([\"year\", \"week\"])[\"price actual\"].mean().reset_index()\n",
    "    weekly_avg_last_year[\"last_year_weekly_mean_price\"] = weekly_avg_last_year.groupby(\"week\")[\"price actual\"].shift(1)\n",
    "    # Determine if a \"last year exists within data, if it does not fill with mean from df_train\"   \n",
    "    if df.loc[df.index[-1], \"time\"] - timedelta(days=365) not in df[\"time\"].values:\n",
    "        weekly_avg_last_year[\"last_year_weekly_mean_price\"].fillna(feature_engineered_data.groupby(feature_engineered_data[\"time\"].dt.isocalendar().week)[\"last_year_weekly_mean_price\"].transform(\"mean\"), inplace=True)\n",
    "    else:\n",
    "        weekly_avg_last_year[\"last_year_weekly_mean_price\"].fillna(weekly_avg_last_year.groupby(\"week\")[\"last_year_weekly_mean_price\"].transform(\"mean\"), inplace=True)\n",
    "\n",
    "    # Create lagged features\n",
    "    for lag in [24, 24*7]: \n",
    "        lag_ma_feats[f\"prev_{'day' if lag == 24 else 'week'}_price\"] = lag_ma_feats[\"price actual\"].shift(lag)\n",
    "        lag_ma_feats[f\"prev_{'day' if lag == 24 else 'week'}_price\"].fillna(lag_ma_feats[\"prev_day_price\"].mean(), inplace=True)\n",
    "\n",
    "    # Create moving averages\n",
    "    for period in [1, 3, 7, 14, 30, 90]:\n",
    "        lag_ma_feats[f\"{period}_day_ma\"] = lag_ma_feats[\"price actual\"].rolling(f\"{period}D\", min_periods=1).mean()\n",
    "\n",
    "    lag_ma_feats = lag_ma_feats.reset_index().merge(weekly_avg_last_year.drop(columns=\"price actual\"), on=[\"year\", \"week\"], how=\"outer\")\n",
    "    lag_ma_feats.drop(columns=['month', 'day','hour','year', 'week'], inplace=True)\n",
    "    #drop response variable\n",
    "    lag_ma_feats.drop(columns=\"price actual\",inplace=True)\n",
    "    merged_time_lag_ma_feat = pd.merge(time_best_feat,lag_ma_feats,on=\"time\",how=\"inner\")\n",
    "    #merge with org. df\n",
    "    df = df.merge(merged_time_lag_ma_feat,on=\"time\",how=\"inner\")\n",
    "    \n",
    "    #Create quadratic trend features\n",
    "    df.reset_index(inplace=True)\n",
    "    df.rename(columns={\"index\":\"t\"},inplace=True)\n",
    "    df[\"t^2\"] = df[\"t\"]**2\n",
    "\n",
    "    df = pd.get_dummies(df,drop_first=True)\n",
    "    return df\n",
    "\n",
    "def apply_transformations(user_data, train_df):\n",
    "    #Int transformers\n",
    "    transformer_scaler = joblib.load(r\"transformers\\scaler.pkl\")\n",
    "    transformer_pca = joblib.load(r\"transformers\\pca.pkl\")\n",
    "\n",
    "    #Response variable\n",
    "    time = user_data[\"time\"]\n",
    "    y = user_data[\"price actual\"]\n",
    "    user_data.drop(columns = [\"time\",\"price actual\"], inplace = True)\n",
    "\n",
    "    #Encode the variables via dummies\n",
    "    user_data = pd.get_dummies(user_data, drop_first=True)\n",
    "    #Interlay missing features based off train_df\n",
    "    user_data = user_data.reindex(columns=train_df.columns, fill_value=False)\n",
    "\n",
    "    #Transform the columns\n",
    "    # For user data, scale numeric columns\n",
    "    numeric_cols_val = user_data.select_dtypes(include=['float64', 'int64']).columns\n",
    "    boolean_cols_train = user_data.select_dtypes(include=['bool']).columns\n",
    "\n",
    "    scaled_numeric_val = transformer_scaler.transform(user_data[numeric_cols_val])\n",
    "\n",
    "    # Create a DataFrame with the scaled numeric data for user data\n",
    "    scaled_numeric_val_df = pd.DataFrame(scaled_numeric_val, columns=numeric_cols_val)\n",
    "\n",
    "    # Combine the scaled numeric DataFrame with the original boolean DataFrame for user data\n",
    "    user_data = pd.concat([scaled_numeric_val_df, user_data[boolean_cols_train].reset_index(drop=True)], axis=1)\n",
    "\n",
    "    #Apply PCA transform\n",
    "    user_data_pca_X = transformer_pca.transform(user_data)\n",
    "\n",
    "    #Converts the pca_data into a dataframe, similar to what was passed for easy manipulation.\n",
    "    columns = [f\"Column {i+1}\" for i in range(user_data_pca_X.shape[1])]\n",
    "    user_data_pca_X = pd.DataFrame(user_data_pca_X, columns=columns)\n",
    "    pca_data_df = pd.concat([time,user_data_pca_X,y],axis=1)\n",
    "    return pca_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "629ce1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "def load_dataset(path):\n",
    "    df = pd.read_csv(path)\n",
    "    if \"time\" in df.columns:\n",
    "        df[\"time\"] = pd.to_datetime(df[\"time\"], utc = True)\n",
    "    return df\n",
    "\n",
    "def manipulate_datasets(energy_df, weather_df):\n",
    "    #Energy Features\n",
    "    energy_df[\"time\"] = pd.to_datetime(energy_df[\"time\"], utc = True)\n",
    "    weather_df[\"time\"] = pd.to_datetime(weather_df[\"time\"], utc = True)\n",
    "    energy_generation_columns = ['generation biomass', 'generation fossil brown coal/lignite',\n",
    "       'generation fossil gas', 'generation fossil hard coal',\n",
    "       'generation fossil oil', 'generation hydro pumped storage consumption',\n",
    "       'generation hydro run-of-river and poundage',\n",
    "       'generation hydro water reservoir', 'generation nuclear',\n",
    "       'generation other', 'generation other renewable', 'generation solar',\n",
    "       'generation waste', 'generation wind onshore'\n",
    "]\n",
    "    energy_df[\"total_gen\"] = energy_df[energy_generation_columns].sum(axis=1)\n",
    "\n",
    "    #Weather data features creation\n",
    "    weather_df[\"temp_diff\"] = weather_df[\"temp_max\"] - weather_df[\"temp_min\"]\n",
    "    weather_df.drop(columns=[\"temp_max\",\"temp_min\"],inplace=True)\n",
    "    #Combine the dfs\n",
    "    cities = weather_df[\"city_name\"].unique()\n",
    "    user_data = energy_df.copy()\n",
    "    for city in cities:\n",
    "        #indexes values for a certain city name and drops the column\n",
    "        df_city = weather_df[weather_df[\"city_name\"] == city]\n",
    "        df_city = df_city.drop(columns=\"city_name\")\n",
    "        \n",
    "        #Duplicate times still exist within the weather dataset with a unique city. To gain a unique time, a \n",
    "        numeric_cols = df_city.select_dtypes(include=['number']).columns\n",
    "        \n",
    "        #Assigns the mean of duplicate time value entries to a unique time value\n",
    "        df_city = df_city.groupby('time')[numeric_cols].agg('mean').reset_index()\n",
    "        \n",
    "        #Rename columns to include the city name\n",
    "        for col in numeric_cols:\n",
    "            df_city = df_city.rename(columns={col: col + \"_\" + city})\n",
    "        \n",
    "        #Merge the dfs\n",
    "        user_data = user_data.merge(df_city, on=\"time\", how=\"inner\")\n",
    "    #Drop columns as no correlation exists as noted in data exploration\n",
    "    try:\n",
    "        user_data.drop(['snow_3h_ Barcelona', 'snow_3h_Seville'], axis=1, inplace=True)\n",
    "    except:\n",
    "        raise st.error(f\"Ensure all five cities are represented within the dataset\")\n",
    "    #Fill nan values with mean\n",
    "    user_data = user_data.fillna(user_data.mean())\n",
    "\n",
    "    return user_data\n",
    "\n",
    "def create_engineered_features(df, feature_engineered_data): \n",
    "    #Time Feats\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    def create_time_features(df, create_time_period_feature = False, step = 6):\n",
    "        #Creates time related features of month, day, hour\n",
    "        time_feat_df = pd.DataFrame()\n",
    "        time_feat_df = df[[\"time\",\"price actual\"]]\n",
    "\n",
    "        time_feat_df[\"month\"] = time_feat_df['time'].dt.month_name()\n",
    "        time_feat_df[\"day\"] = time_feat_df['time'].dt.day_name()\n",
    "        time_feat_df[\"hour\"] = time_feat_df[\"time\"].dt.hour.astype('category')\n",
    "        #Creates stepped features based on hour\n",
    "        #using pd.getdummies creates 23 additional features, whereas hour features could be captured via a period reducing dimensionality\n",
    "        if create_time_period_feature:\n",
    "            for i in range(step, 24, step):\n",
    "                time_feat_df[f\"is_btwn_{i}_and_{i+step}\"] = time_feat_df[\"hour\"].isin(range(i, i + step)).astype(bool)\n",
    "\n",
    "            time_feat_df.drop(columns=[\"hour\"],inplace=True)\n",
    "        \n",
    "        return time_feat_df\n",
    "    time_best_feat = create_time_features(df, create_time_period_feature=True,step = 6).drop(columns=\"price actual\")\n",
    "\n",
    "#Lag and MA feats\n",
    "# Create time features\n",
    "    lag_ma_feats = create_time_features(df)\n",
    "    lag_ma_feats[\"year\"] = lag_ma_feats[\"time\"].dt.year\n",
    "    lag_ma_feats[\"week\"] = lag_ma_feats[\"time\"].dt.isocalendar().week\n",
    "    lag_ma_feats.set_index(\"time\", inplace=True)\n",
    "\n",
    "    # Compute weekly average and merge with previous years for comparison\n",
    "   \n",
    "    weekly_avg_last_year = lag_ma_feats.groupby([\"year\", \"week\"])[\"price actual\"].mean().reset_index()\n",
    "    weekly_avg_last_year[\"last_year_weekly_mean_price\"] = weekly_avg_last_year.groupby(\"week\")[\"price actual\"].shift(1)\n",
    "    # Determine if a \"last year exists within data, if it does not fill with mean from df_train\"   \n",
    "    if df.loc[df.index[-1], \"time\"] - timedelta(days=365) not in df[\"time\"].values:\n",
    "        weekly_avg_last_year[\"last_year_weekly_mean_price\"].fillna(feature_engineered_data.groupby(feature_engineered_data[\"time\"].dt.isocalendar().week)[\"last_year_weekly_mean_price\"].transform(\"mean\"), inplace=True)\n",
    "    else:\n",
    "        weekly_avg_last_year[\"last_year_weekly_mean_price\"].fillna(weekly_avg_last_year.groupby(\"week\")[\"last_year_weekly_mean_price\"].transform(\"mean\"), inplace=True)\n",
    "\n",
    "    # Create lagged features\n",
    "    for lag in [24, 24*7]: \n",
    "        lag_ma_feats[f\"prev_{'day' if lag == 24 else 'week'}_price\"] = lag_ma_feats[\"price actual\"].shift(lag)\n",
    "        lag_ma_feats[f\"prev_{'day' if lag == 24 else 'week'}_price\"].fillna(lag_ma_feats[\"prev_day_price\"].mean(), inplace=True)\n",
    "\n",
    "    # Create moving averages\n",
    "    for period in [1, 3, 7, 14, 30, 90]:\n",
    "        lag_ma_feats[f\"{period}_day_ma\"] = lag_ma_feats[\"price actual\"].rolling(f\"{period}D\", min_periods=1).mean()\n",
    "\n",
    "    lag_ma_feats = lag_ma_feats.reset_index().merge(weekly_avg_last_year.drop(columns=\"price actual\"), on=[\"year\", \"week\"], how=\"outer\")\n",
    "    lag_ma_feats.drop(columns=['month', 'day','hour','year', 'week'], inplace=True)\n",
    "    lag_ma_feats.drop(columns=\"price actual\",inplace=True)\n",
    "    #Create a combined dataset\n",
    "    merged_time_lag_ma_feat = pd.merge(time_best_feat,lag_ma_feats,on=\"time\",how=\"inner\")\n",
    "\n",
    "    df = df.merge(merged_time_lag_ma_feat,on=\"time\",how=\"inner\")\n",
    "    \n",
    "    #Create quadratic trend features\n",
    "    df.reset_index(inplace=True)\n",
    "    df.rename(columns={\"index\":\"t\"},inplace=True)\n",
    "    df[\"t^2\"] = df[\"t\"]**2\n",
    "    # Create dummy variablse for non-numeric values while avoid co-linearity\n",
    "    #df = pd.get_dummies(df,drop_first=True)\n",
    "    return df\n",
    "\n",
    "def apply_transformations(user_data, train_df):\n",
    "    #Int transformers\n",
    "    transformer_scaler = joblib.load(r\"transformers\\scaler.pkl\")\n",
    "    transformer_pca = joblib.load(r\"transformers\\pca.pkl\")\n",
    "\n",
    "    #Create variables for storing time, response variable, and predictors\n",
    "    time = user_data[\"time\"]\n",
    "    y = user_data[\"price actual\"]\n",
    "    user_data.drop(columns = [\"time\",\"price actual\"], inplace = True)\n",
    "\n",
    "    #Encode the variables via dummies\n",
    "    user_data = pd.get_dummies(user_data, drop_first=True)\n",
    "    #Interlay missing features based off train_df\n",
    "    user_data = user_data.reindex(columns=train_df.columns, fill_value=False)\n",
    "\n",
    "    #Transform the columns\n",
    "    # For user data, scale numeric columns\n",
    "    numeric_cols_val = user_data.select_dtypes(include=['float64', 'int64']).columns\n",
    "    boolean_cols_train = user_data.select_dtypes(include=['bool']).columns\n",
    "\n",
    "    scaled_numeric_val = transformer_scaler.transform(user_data[numeric_cols_val])\n",
    "\n",
    "    # Create a DataFrame with the scaled numeric data for user data\n",
    "    scaled_numeric_val_df = pd.DataFrame(scaled_numeric_val, columns=numeric_cols_val)\n",
    "\n",
    "    # Combine the scaled numeric DataFrame with the original boolean DataFrame for user data\n",
    "    user_data = pd.concat([scaled_numeric_val_df, user_data[boolean_cols_train].reset_index(drop=True)], axis=1)\n",
    "\n",
    "    #Apply PCA transform\n",
    "    user_data_pca_X = transformer_pca.transform(user_data)\n",
    "\n",
    "    #Converts the pca_data into a dataframe, similar to what was passed for easy manipulation.\n",
    "    columns = [f\"Column {i+1}\" for i in range(user_data_pca_X.shape[1])]\n",
    "    user_data_pca_X = pd.DataFrame(user_data_pca_X, columns=columns)\n",
    "    pca_data_df = pd.concat([time,user_data_pca_X,y],axis=1)\n",
    "    return pca_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b85f592a",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df = pd.read_csv(r\"engineered_data\\weather_data_val_manipulated.csv\")\n",
    "energy_df = pd.read_csv(r\"engineered_data\\energy_data_val_manipulated.csv\")\n",
    "combined_data = load_dataset(r\"engineered_data\\transformed_data_first_rev.csv\")\n",
    "engineered_features_data = load_dataset(r\"engineered_data\\feature_engineered_data.csv\")\n",
    "pca_data = load_dataset(\"engineered_data\\pca_ml_ready_data.csv\")\n",
    "ml_data_X_train = load_dataset(r\"engineered_data\\feature_engineered_data_X_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f6db06af",
   "metadata": {},
   "outputs": [],
   "source": [
    "deploy_df = manipulate_datasets(energy_df.copy(),weather_df.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "67964097",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7013"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(deploy_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "67c76ed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing columns from deploy_df compared to target_df: set()\n",
      "Number of missing columns from deploy_df compared to target_df: 0\n",
      "Missing columns from target_df compared to deploy_df: set()\n",
      "Number of missing columns from target_df compared to deploy_df: 0\n",
      "Number of columns in deploy_df: 75\n",
      "Number of columns in target_df: 75\n",
      "The number of columns in deploy_df is equal to that in target_df.\n"
     ]
    }
   ],
   "source": [
    "#Determine if manipulate_datasets outputs the same columns and features as combined_data\n",
    "compare_columns_df(deploy_df,combined_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "34af2287",
   "metadata": {},
   "outputs": [],
   "source": [
    "deploy_feature_engineered_df = create_engineered_features(deploy_df.copy(),engineered_features_data.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eb2efd13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing columns from deploy_df compared to target_df: {'price actual', 'time'}\n",
      "Number of missing columns from deploy_df compared to target_df: 2\n",
      "Missing columns from target_df compared to deploy_df: set()\n",
      "Number of missing columns from target_df compared to deploy_df: 0\n",
      "Number of columns in deploy_df: 89\n",
      "Number of columns in target_df: 91\n",
      "The number of columns in deploy_df is not equal to that in target_df.\n"
     ]
    }
   ],
   "source": [
    "compare_columns_df(deploy_feature_engineered_df.drop(columns=[\"price actual\", \"time\"]),engineered_features_data.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7807230f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7013 entries, 0 to 7012\n",
      "Data columns (total 99 columns):\n",
      " #   Column                                       Non-Null Count  Dtype              \n",
      "---  ------                                       --------------  -----              \n",
      " 0   t                                            7013 non-null   int64              \n",
      " 1   time                                         7013 non-null   datetime64[ns, UTC]\n",
      " 2   generation biomass                           7013 non-null   float64            \n",
      " 3   generation fossil brown coal/lignite         7013 non-null   float64            \n",
      " 4   generation fossil gas                        7013 non-null   float64            \n",
      " 5   generation fossil hard coal                  7013 non-null   float64            \n",
      " 6   generation fossil oil                        7013 non-null   float64            \n",
      " 7   generation hydro pumped storage consumption  7013 non-null   float64            \n",
      " 8   generation hydro run-of-river and poundage   7013 non-null   float64            \n",
      " 9   generation hydro water reservoir             7013 non-null   float64            \n",
      " 10  generation nuclear                           7013 non-null   float64            \n",
      " 11  generation other                             7013 non-null   float64            \n",
      " 12  generation other renewable                   7013 non-null   float64            \n",
      " 13  generation solar                             7013 non-null   float64            \n",
      " 14  generation waste                             7013 non-null   float64            \n",
      " 15  generation wind onshore                      7013 non-null   float64            \n",
      " 16  forecast solar day ahead                     7013 non-null   float64            \n",
      " 17  forecast wind onshore day ahead              7013 non-null   float64            \n",
      " 18  total load forecast                          7013 non-null   float64            \n",
      " 19  total load actual                            7013 non-null   float64            \n",
      " 20  price day ahead                              7013 non-null   float64            \n",
      " 21  price actual                                 7013 non-null   float64            \n",
      " 22  total_gen                                    7013 non-null   float64            \n",
      " 23  temp_Valencia                                7013 non-null   float64            \n",
      " 24  pressure_Valencia                            7013 non-null   float64            \n",
      " 25  humidity_Valencia                            7013 non-null   float64            \n",
      " 26  wind_speed_Valencia                          7013 non-null   float64            \n",
      " 27  wind_deg_Valencia                            7013 non-null   float64            \n",
      " 28  rain_1h_Valencia                             7013 non-null   float64            \n",
      " 29  rain_3h_Valencia                             7013 non-null   float64            \n",
      " 30  snow_3h_Valencia                             7013 non-null   float64            \n",
      " 31  clouds_all_Valencia                          7013 non-null   float64            \n",
      " 32  temp_diff_Valencia                           7013 non-null   float64            \n",
      " 33  temp_Madrid                                  7013 non-null   float64            \n",
      " 34  pressure_Madrid                              7013 non-null   float64            \n",
      " 35  humidity_Madrid                              7013 non-null   float64            \n",
      " 36  wind_speed_Madrid                            7013 non-null   float64            \n",
      " 37  wind_deg_Madrid                              7013 non-null   float64            \n",
      " 38  rain_1h_Madrid                               7013 non-null   float64            \n",
      " 39  rain_3h_Madrid                               7013 non-null   float64            \n",
      " 40  snow_3h_Madrid                               7013 non-null   float64            \n",
      " 41  clouds_all_Madrid                            7013 non-null   float64            \n",
      " 42  temp_diff_Madrid                             7013 non-null   float64            \n",
      " 43  temp_Bilbao                                  7013 non-null   float64            \n",
      " 44  pressure_Bilbao                              7013 non-null   float64            \n",
      " 45  humidity_Bilbao                              7013 non-null   float64            \n",
      " 46  wind_speed_Bilbao                            7013 non-null   float64            \n",
      " 47  wind_deg_Bilbao                              7013 non-null   float64            \n",
      " 48  rain_1h_Bilbao                               7013 non-null   float64            \n",
      " 49  rain_3h_Bilbao                               7013 non-null   float64            \n",
      " 50  snow_3h_Bilbao                               7013 non-null   float64            \n",
      " 51  clouds_all_Bilbao                            7013 non-null   float64            \n",
      " 52  temp_diff_Bilbao                             7013 non-null   float64            \n",
      " 53  temp_ Barcelona                              7013 non-null   float64            \n",
      " 54  pressure_ Barcelona                          7013 non-null   float64            \n",
      " 55  humidity_ Barcelona                          7013 non-null   float64            \n",
      " 56  wind_speed_ Barcelona                        7013 non-null   float64            \n",
      " 57  wind_deg_ Barcelona                          7013 non-null   float64            \n",
      " 58  rain_1h_ Barcelona                           7013 non-null   float64            \n",
      " 59  rain_3h_ Barcelona                           7013 non-null   float64            \n",
      " 60  clouds_all_ Barcelona                        7013 non-null   float64            \n",
      " 61  temp_diff_ Barcelona                         7013 non-null   float64            \n",
      " 62  temp_Seville                                 7013 non-null   float64            \n",
      " 63  pressure_Seville                             7013 non-null   float64            \n",
      " 64  humidity_Seville                             7013 non-null   float64            \n",
      " 65  wind_speed_Seville                           7013 non-null   float64            \n",
      " 66  wind_deg_Seville                             7013 non-null   float64            \n",
      " 67  rain_1h_Seville                              7013 non-null   float64            \n",
      " 68  rain_3h_Seville                              7013 non-null   float64            \n",
      " 69  clouds_all_Seville                           7013 non-null   float64            \n",
      " 70  temp_diff_Seville                            7013 non-null   float64            \n",
      " 71  is_btwn_6_and_12                             7013 non-null   bool               \n",
      " 72  is_btwn_12_and_18                            7013 non-null   bool               \n",
      " 73  is_btwn_18_and_24                            7013 non-null   bool               \n",
      " 74  prev_day_price                               7013 non-null   float64            \n",
      " 75  prev_week_price                              7013 non-null   float64            \n",
      " 76  1_day_ma                                     7013 non-null   float64            \n",
      " 77  3_day_ma                                     7013 non-null   float64            \n",
      " 78  7_day_ma                                     7013 non-null   float64            \n",
      " 79  14_day_ma                                    7013 non-null   float64            \n",
      " 80  30_day_ma                                    7013 non-null   float64            \n",
      " 81  90_day_ma                                    7013 non-null   float64            \n",
      " 82  last_year_weekly_mean_price                  7013 non-null   float64            \n",
      " 83  t^2                                          7013 non-null   int64              \n",
      " 84  month_August                                 7013 non-null   bool               \n",
      " 85  month_December                               7013 non-null   bool               \n",
      " 86  month_July                                   7013 non-null   bool               \n",
      " 87  month_June                                   7013 non-null   bool               \n",
      " 88  month_March                                  7013 non-null   bool               \n",
      " 89  month_May                                    7013 non-null   bool               \n",
      " 90  month_November                               7013 non-null   bool               \n",
      " 91  month_October                                7013 non-null   bool               \n",
      " 92  month_September                              7013 non-null   bool               \n",
      " 93  day_Monday                                   7013 non-null   bool               \n",
      " 94  day_Saturday                                 7013 non-null   bool               \n",
      " 95  day_Sunday                                   7013 non-null   bool               \n",
      " 96  day_Thursday                                 7013 non-null   bool               \n",
      " 97  day_Tuesday                                  7013 non-null   bool               \n",
      " 98  day_Wednesday                                7013 non-null   bool               \n",
      "dtypes: bool(18), datetime64[ns, UTC](1), float64(78), int64(2)\n",
      "memory usage: 4.5 MB\n"
     ]
    }
   ],
   "source": [
    "deploy_feature_engineered_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "62c8a027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is_btwn_6_and_12\n",
      "is_btwn_12_and_18\n",
      "is_btwn_18_and_24\n",
      "prev_day_price\n",
      "prev_week_price\n",
      "1_day_ma\n",
      "3_day_ma\n",
      "7_day_ma\n",
      "14_day_ma\n",
      "30_day_ma\n",
      "90_day_ma\n",
      "last_year_weekly_mean_price\n",
      "t^2\n",
      "month_July\n",
      "month_June\n",
      "month_March\n",
      "month_May\n",
      "month_November\n",
      "month_October\n",
      "month_September\n",
      "day_Monday\n",
      "day_Saturday\n",
      "day_Sunday\n",
      "day_Thursday\n",
      "day_Tuesday\n",
      "day_Wednesday\n"
     ]
    }
   ],
   "source": [
    "for p, q in zip(deploy_feature_engineered_df.drop(columns=[\"time\", \"price actual\"]).columns, ml_data_X_train.columns):\n",
    "    if p != q:\n",
    "        print(p)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "368b0f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "deploy_feature_engineered_pca = apply_transformations(deploy_feature_engineered_df.copy(),ml_data_X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "513a278b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing columns from deploy_df compared to target_df: set()\n",
      "Number of missing columns from deploy_df compared to target_df: 0\n",
      "Missing columns from target_df compared to deploy_df: set()\n",
      "Number of missing columns from target_df compared to deploy_df: 0\n",
      "Number of columns in deploy_df: 61\n",
      "Number of columns in target_df: 61\n",
      "The number of columns in deploy_df is equal to that in target_df.\n"
     ]
    }
   ],
   "source": [
    "compare_columns_df(deploy_feature_engineered_pca,pca_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6681b6a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>Column 1</th>\n",
       "      <th>Column 2</th>\n",
       "      <th>Column 3</th>\n",
       "      <th>Column 4</th>\n",
       "      <th>Column 5</th>\n",
       "      <th>Column 6</th>\n",
       "      <th>Column 7</th>\n",
       "      <th>Column 8</th>\n",
       "      <th>Column 9</th>\n",
       "      <th>...</th>\n",
       "      <th>Column 47</th>\n",
       "      <th>Column 48</th>\n",
       "      <th>Column 49</th>\n",
       "      <th>Column 50</th>\n",
       "      <th>Column 51</th>\n",
       "      <th>Column 52</th>\n",
       "      <th>Column 53</th>\n",
       "      <th>Column 54</th>\n",
       "      <th>Column 55</th>\n",
       "      <th>price actual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-03-14 18:00:00+00:00</td>\n",
       "      <td>-4.274857</td>\n",
       "      <td>0.825348</td>\n",
       "      <td>4.641214</td>\n",
       "      <td>8.804337</td>\n",
       "      <td>-1.959678</td>\n",
       "      <td>-1.889252</td>\n",
       "      <td>2.955439</td>\n",
       "      <td>2.872822</td>\n",
       "      <td>1.500136</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.187850</td>\n",
       "      <td>0.897823</td>\n",
       "      <td>-1.595161</td>\n",
       "      <td>-0.201287</td>\n",
       "      <td>1.885004</td>\n",
       "      <td>0.050559</td>\n",
       "      <td>0.196497</td>\n",
       "      <td>-0.144339</td>\n",
       "      <td>-0.369273</td>\n",
       "      <td>50.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-03-14 19:00:00+00:00</td>\n",
       "      <td>-2.960907</td>\n",
       "      <td>0.772773</td>\n",
       "      <td>3.585117</td>\n",
       "      <td>7.192523</td>\n",
       "      <td>-0.706520</td>\n",
       "      <td>-0.902016</td>\n",
       "      <td>2.667317</td>\n",
       "      <td>2.342305</td>\n",
       "      <td>-0.053881</td>\n",
       "      <td>...</td>\n",
       "      <td>0.289333</td>\n",
       "      <td>0.702527</td>\n",
       "      <td>-0.529769</td>\n",
       "      <td>0.212551</td>\n",
       "      <td>1.805751</td>\n",
       "      <td>-0.078712</td>\n",
       "      <td>-0.148836</td>\n",
       "      <td>-0.160198</td>\n",
       "      <td>-0.435772</td>\n",
       "      <td>59.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-03-14 20:00:00+00:00</td>\n",
       "      <td>-3.443623</td>\n",
       "      <td>0.035503</td>\n",
       "      <td>3.852856</td>\n",
       "      <td>6.984090</td>\n",
       "      <td>-1.501447</td>\n",
       "      <td>-1.348218</td>\n",
       "      <td>2.493640</td>\n",
       "      <td>2.403798</td>\n",
       "      <td>0.083945</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.622341</td>\n",
       "      <td>1.600989</td>\n",
       "      <td>-0.244156</td>\n",
       "      <td>0.725674</td>\n",
       "      <td>0.909233</td>\n",
       "      <td>0.553918</td>\n",
       "      <td>0.612848</td>\n",
       "      <td>0.016350</td>\n",
       "      <td>-0.384135</td>\n",
       "      <td>50.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-03-14 21:00:00+00:00</td>\n",
       "      <td>-3.583661</td>\n",
       "      <td>-0.725679</td>\n",
       "      <td>3.110287</td>\n",
       "      <td>4.842457</td>\n",
       "      <td>-1.032246</td>\n",
       "      <td>-1.228726</td>\n",
       "      <td>1.865396</td>\n",
       "      <td>3.186572</td>\n",
       "      <td>0.361537</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.723466</td>\n",
       "      <td>0.966384</td>\n",
       "      <td>-0.136178</td>\n",
       "      <td>0.504955</td>\n",
       "      <td>1.021985</td>\n",
       "      <td>0.804953</td>\n",
       "      <td>0.823498</td>\n",
       "      <td>0.601754</td>\n",
       "      <td>-0.136876</td>\n",
       "      <td>45.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-03-14 22:00:00+00:00</td>\n",
       "      <td>-5.375297</td>\n",
       "      <td>-0.435624</td>\n",
       "      <td>4.353312</td>\n",
       "      <td>6.264673</td>\n",
       "      <td>-2.233537</td>\n",
       "      <td>-0.621059</td>\n",
       "      <td>2.596650</td>\n",
       "      <td>1.793152</td>\n",
       "      <td>0.959746</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.726433</td>\n",
       "      <td>0.757391</td>\n",
       "      <td>-0.894388</td>\n",
       "      <td>1.348849</td>\n",
       "      <td>1.225874</td>\n",
       "      <td>0.341591</td>\n",
       "      <td>1.140301</td>\n",
       "      <td>0.913555</td>\n",
       "      <td>-0.036593</td>\n",
       "      <td>30.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7008</th>\n",
       "      <td>2018-12-31 18:00:00+00:00</td>\n",
       "      <td>2.226959</td>\n",
       "      <td>-2.483943</td>\n",
       "      <td>-2.149679</td>\n",
       "      <td>-0.989503</td>\n",
       "      <td>2.225141</td>\n",
       "      <td>0.871963</td>\n",
       "      <td>-0.183559</td>\n",
       "      <td>1.154734</td>\n",
       "      <td>0.281908</td>\n",
       "      <td>...</td>\n",
       "      <td>0.751888</td>\n",
       "      <td>-0.287421</td>\n",
       "      <td>-0.673239</td>\n",
       "      <td>0.727310</td>\n",
       "      <td>1.134760</td>\n",
       "      <td>-0.088459</td>\n",
       "      <td>0.544819</td>\n",
       "      <td>-0.263466</td>\n",
       "      <td>-0.525234</td>\n",
       "      <td>77.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7009</th>\n",
       "      <td>2018-12-31 19:00:00+00:00</td>\n",
       "      <td>2.088942</td>\n",
       "      <td>-2.801025</td>\n",
       "      <td>-2.163501</td>\n",
       "      <td>-1.203789</td>\n",
       "      <td>2.344032</td>\n",
       "      <td>1.196169</td>\n",
       "      <td>0.104872</td>\n",
       "      <td>0.640753</td>\n",
       "      <td>0.170250</td>\n",
       "      <td>...</td>\n",
       "      <td>1.119913</td>\n",
       "      <td>0.507457</td>\n",
       "      <td>-0.231988</td>\n",
       "      <td>0.630862</td>\n",
       "      <td>1.455573</td>\n",
       "      <td>-0.532136</td>\n",
       "      <td>0.544907</td>\n",
       "      <td>-0.318163</td>\n",
       "      <td>0.143101</td>\n",
       "      <td>76.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7010</th>\n",
       "      <td>2018-12-31 20:00:00+00:00</td>\n",
       "      <td>1.689580</td>\n",
       "      <td>-3.428798</td>\n",
       "      <td>-1.900630</td>\n",
       "      <td>-0.896439</td>\n",
       "      <td>1.872678</td>\n",
       "      <td>1.700217</td>\n",
       "      <td>-0.691526</td>\n",
       "      <td>-0.325952</td>\n",
       "      <td>0.136372</td>\n",
       "      <td>...</td>\n",
       "      <td>1.499733</td>\n",
       "      <td>0.230813</td>\n",
       "      <td>-0.214316</td>\n",
       "      <td>1.082689</td>\n",
       "      <td>1.104597</td>\n",
       "      <td>-0.548814</td>\n",
       "      <td>1.096338</td>\n",
       "      <td>-0.314873</td>\n",
       "      <td>-0.186263</td>\n",
       "      <td>74.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7011</th>\n",
       "      <td>2018-12-31 21:00:00+00:00</td>\n",
       "      <td>1.275254</td>\n",
       "      <td>-3.734704</td>\n",
       "      <td>-1.947809</td>\n",
       "      <td>-1.336982</td>\n",
       "      <td>1.604230</td>\n",
       "      <td>1.981913</td>\n",
       "      <td>-0.032176</td>\n",
       "      <td>-0.825763</td>\n",
       "      <td>0.074386</td>\n",
       "      <td>...</td>\n",
       "      <td>1.411348</td>\n",
       "      <td>0.777585</td>\n",
       "      <td>-0.141087</td>\n",
       "      <td>1.059724</td>\n",
       "      <td>0.987899</td>\n",
       "      <td>-0.606894</td>\n",
       "      <td>1.063430</td>\n",
       "      <td>-0.333171</td>\n",
       "      <td>-0.241595</td>\n",
       "      <td>69.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7012</th>\n",
       "      <td>2018-12-31 22:00:00+00:00</td>\n",
       "      <td>1.132222</td>\n",
       "      <td>-4.672487</td>\n",
       "      <td>-2.108006</td>\n",
       "      <td>-1.650348</td>\n",
       "      <td>1.237076</td>\n",
       "      <td>1.496947</td>\n",
       "      <td>0.234224</td>\n",
       "      <td>-0.697786</td>\n",
       "      <td>0.018322</td>\n",
       "      <td>...</td>\n",
       "      <td>1.950317</td>\n",
       "      <td>-0.509915</td>\n",
       "      <td>-0.372779</td>\n",
       "      <td>0.716323</td>\n",
       "      <td>0.890402</td>\n",
       "      <td>-0.041052</td>\n",
       "      <td>0.374768</td>\n",
       "      <td>0.130561</td>\n",
       "      <td>-0.601071</td>\n",
       "      <td>69.88</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7013 rows × 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          time  Column 1  Column 2  Column 3  Column 4  \\\n",
       "0    2018-03-14 18:00:00+00:00 -4.274857  0.825348  4.641214  8.804337   \n",
       "1    2018-03-14 19:00:00+00:00 -2.960907  0.772773  3.585117  7.192523   \n",
       "2    2018-03-14 20:00:00+00:00 -3.443623  0.035503  3.852856  6.984090   \n",
       "3    2018-03-14 21:00:00+00:00 -3.583661 -0.725679  3.110287  4.842457   \n",
       "4    2018-03-14 22:00:00+00:00 -5.375297 -0.435624  4.353312  6.264673   \n",
       "...                        ...       ...       ...       ...       ...   \n",
       "7008 2018-12-31 18:00:00+00:00  2.226959 -2.483943 -2.149679 -0.989503   \n",
       "7009 2018-12-31 19:00:00+00:00  2.088942 -2.801025 -2.163501 -1.203789   \n",
       "7010 2018-12-31 20:00:00+00:00  1.689580 -3.428798 -1.900630 -0.896439   \n",
       "7011 2018-12-31 21:00:00+00:00  1.275254 -3.734704 -1.947809 -1.336982   \n",
       "7012 2018-12-31 22:00:00+00:00  1.132222 -4.672487 -2.108006 -1.650348   \n",
       "\n",
       "      Column 5  Column 6  Column 7  Column 8  Column 9  ...  Column 47  \\\n",
       "0    -1.959678 -1.889252  2.955439  2.872822  1.500136  ...  -0.187850   \n",
       "1    -0.706520 -0.902016  2.667317  2.342305 -0.053881  ...   0.289333   \n",
       "2    -1.501447 -1.348218  2.493640  2.403798  0.083945  ...  -0.622341   \n",
       "3    -1.032246 -1.228726  1.865396  3.186572  0.361537  ...  -0.723466   \n",
       "4    -2.233537 -0.621059  2.596650  1.793152  0.959746  ...  -0.726433   \n",
       "...        ...       ...       ...       ...       ...  ...        ...   \n",
       "7008  2.225141  0.871963 -0.183559  1.154734  0.281908  ...   0.751888   \n",
       "7009  2.344032  1.196169  0.104872  0.640753  0.170250  ...   1.119913   \n",
       "7010  1.872678  1.700217 -0.691526 -0.325952  0.136372  ...   1.499733   \n",
       "7011  1.604230  1.981913 -0.032176 -0.825763  0.074386  ...   1.411348   \n",
       "7012  1.237076  1.496947  0.234224 -0.697786  0.018322  ...   1.950317   \n",
       "\n",
       "      Column 48  Column 49  Column 50  Column 51  Column 52  Column 53  \\\n",
       "0      0.897823  -1.595161  -0.201287   1.885004   0.050559   0.196497   \n",
       "1      0.702527  -0.529769   0.212551   1.805751  -0.078712  -0.148836   \n",
       "2      1.600989  -0.244156   0.725674   0.909233   0.553918   0.612848   \n",
       "3      0.966384  -0.136178   0.504955   1.021985   0.804953   0.823498   \n",
       "4      0.757391  -0.894388   1.348849   1.225874   0.341591   1.140301   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "7008  -0.287421  -0.673239   0.727310   1.134760  -0.088459   0.544819   \n",
       "7009   0.507457  -0.231988   0.630862   1.455573  -0.532136   0.544907   \n",
       "7010   0.230813  -0.214316   1.082689   1.104597  -0.548814   1.096338   \n",
       "7011   0.777585  -0.141087   1.059724   0.987899  -0.606894   1.063430   \n",
       "7012  -0.509915  -0.372779   0.716323   0.890402  -0.041052   0.374768   \n",
       "\n",
       "      Column 54  Column 55  price actual  \n",
       "0     -0.144339  -0.369273         50.95  \n",
       "1     -0.160198  -0.435772         59.73  \n",
       "2      0.016350  -0.384135         50.89  \n",
       "3      0.601754  -0.136876         45.94  \n",
       "4      0.913555  -0.036593         30.59  \n",
       "...         ...        ...           ...  \n",
       "7008  -0.263466  -0.525234         77.02  \n",
       "7009  -0.318163   0.143101         76.16  \n",
       "7010  -0.314873  -0.186263         74.30  \n",
       "7011  -0.333171  -0.241595         69.89  \n",
       "7012   0.130561  -0.601071         69.88  \n",
       "\n",
       "[7013 rows x 57 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deploy_feature_engineered_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c9d227a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = joblib.load(r\"models\\best_model_linear_reg.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1200ec61",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(deploy_feature_engineered_pca.drop(columns=[\"time\", \"price actual\"]))\n",
    "y_val = deploy_feature_engineered_pca[\"price actual\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4acc35fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67.6105065223949"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
